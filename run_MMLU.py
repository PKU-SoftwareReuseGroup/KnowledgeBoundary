# å‚è€ƒ R-tuning é¡¹ç›® https://github.com/shizhediao/R-Tuning/
import json
import os
import random
from argparse import ArgumentParser
from sys import exit

import numpy as np
import torch
from tqdm.auto import tqdm
# å¼•å…¥è‡ªå®šä¹‰å˜é‡
from constants import get_TOKENIZER_and_MODEL

choices = ["A", "B", "C", "D"]


def format_example(input_list):
    prompt = input_list[0]
    k = len(input_list) - 2
    for j in range(k):
        prompt += f"\n{choices[j]}. {input_list[j+1]}"
    prompt += "\nAnswer:"
    return prompt


def format_shots(prompt_data):
    prompt = ""
    for data in prompt_data:
        prompt += data[0]       # fewshot çš„ Question
        k = len(data) - 2       # fewshot ä¸­ä½œä¸ºé€‰é¡¹çš„ Answer çš„ä¸ªæ•°
        for j in range(k):      # æ ¼å¼åŒ–åŠ å…¥ A B C D å››ä¸ªé€‰é¡¹
            prompt += f"\n{choices[j]}. {data[j+1]}"
        prompt += "\nAnswer:"
        prompt += data[k+1] + "\n\n"

    return prompt


def gen_prompt(input_list: list[str], subject:str, prompt_data: list[list[str]]):
    """ åŸºäº MMLU æ•°æ®ç”Ÿæˆ prompt
    è¿™ä¸ª prompt æ˜¯é’ˆå¯¹ â€œå¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰è¢«è°ƒæˆä¸€ä¸ª chatbotâ€ çš„æƒ…å†µã€‚æ‰€ä»¥ä¸åº”è¯¥åŠ å…¥ æŒ‡ä»¤æ€§è¯­å¥
    è€Œæ˜¯ä»¥ fewshots çš„å½¢å¼è®©æ¨¡å‹è¿›è¡Œ text completion/genertion
    Params:
        input_text: MMLU çš„å¤šé€‰é¢˜æ•°æ® Question, Option1, Option2, Option3, Option4, Answer
        subject: MMLU çš„ keyâ€”â€”é¢†åŸŸåç§°
        prompt_data: prompt.json åœ¨ subject é¢†åŸŸä¸‹çš„å†…å®¹
    """
    # NOTE é¢†åŸŸä»‹ç»
    # prompt = f"The following are multiple choice questions (with answers) about{subject}.\n\n"
    prompt = ""
    # NOTE fewshot æ„å»º
    prompt += format_shots(prompt_data)
    # NOTE é—®é¢˜åŠ å…¥
    prompt += format_example(input_list)
    return prompt


# Qwen2.5-3B ç”¨å®Œæ•´çš„ 5-shot prompt æ— æ³•ç”Ÿæˆç»“æœï¼Œä¸”å›ç­”æ€»æ˜¯å…ˆç†ç”±å†é€‰é¡¹ã€‚ç”¨ 1-shot + æŒ‡ä»¤è§„èŒƒ
def gen_one_shot_prompt(input_list: list[str], subject:str, data: list[str]):
    # NOTE é¢†åŸŸä»‹ç»
    prompt = f"The following are multiple choice questions (with answers) about{subject}.\n\n"
    # NOTE oneshot æ„å»ºï¼Œè¿™é‡Œå›ºå®šå…ˆå›ç­”é€‰é¡¹ï¼Œå†å›ç­”å…¶ä»–
    prompt += data[0]
    k = len(data) - 2
    for j in range(k):
        prompt += f"\n{choices[j]}. {data[j+1]}"
    prompt += f"\nAnswer: {data[k+1]}.\n\n"
    # NOTE æ¨¡å‹æ±‚è§£é—®é¢˜æ„å»ºï¼Œæˆ‘è¿™é‡ŒåŠ å…¥äº†æŒ‡å¯¼æ€§è¯­å¥
    prompt += "Now, there is a question for you: \n"
    prompt += input_list[0]
    k = len(input_list) - 2
    for j in range(k):
        prompt += f"\n{choices[j]}. {input_list[j+1]}"

    prompt += "\nYour task: Firstly, choose the correct answer from options A, B, C, and D. Secondly, give reasons.\n"
    return prompt


def inference(
        tokenizer, 
        model, 
        full_input: str, 
        subject:str
    ):
    """
    Params:
        tokenizer: åˆ†è¯å™¨
        model: ç”¨äºæ¨ç†çš„æ¨¡å‹
        full_input: 5-shots prompt æˆ–è€… 1-shot prompt
        subject: é¢†åŸŸåç§°
    """
    output_text = ""
    # å»æ‰ MMLU å­é¢†åŸŸä¸­çš„ä¸‹åˆ’çº¿ï¼ŒåŠ ç©ºæ ¼
    l = subject.split("_")
    s = ""
    for entry in l:
        s += " " + entry

    if args.model == "openlm-research/open_llama_3b":
        inputs = tokenizer(full_input,return_tensors="pt").to(0)
        ids = inputs['input_ids']
        length = len(ids[0])
        # è¾“å‡ºæœ‰ä¸‰ä¸ªé”®å€¼ ['sequences', 'scores', 'past_key_values']
        outputs = model.generate(
                ids,
                # NOTE è¿™é‡Œé™åˆ¶æ–°ç”Ÿæˆçš„ token é•¿åº¦ä¸º1ï¼Œå…¶å®å¹¶ä¸èƒ½ä¿è¯ç”Ÿæˆä¸€ä¸ªå­—æ¯ï¼Œå¯èƒ½æ˜¯å…¶ä»–å­—ç¬¦
                max_new_tokens = 20,
                output_scores = True,   # è¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ª token çš„åˆ†æ•°ï¼ˆlogitsï¼‰
                return_dict_in_generate=True    # ä»¥å­—å…¸å½¢å¼è¿”å›ç”Ÿæˆçš„è¾“å‡ºï¼Œå…¶ä¸­åŒ…æ‹¬ç”Ÿæˆçš„ tokens å’Œ logits
            )
        # print(tokenizer.decode(outputs['sequences'][0][length:]))
        # logits æ˜¯æ¨¡å‹è¾“å‡ºçš„å¯¹æ¯ä¸ªå¯èƒ½çš„ token çš„åŸå§‹åˆ†æ•°ï¼Œé€šå¸¸æ˜¯ä¸€ä¸ªå‘é‡ï¼Œè¡¨ç¤ºæ‰€æœ‰è¯æ±‡è¡¨ä¸­æ¯ä¸ª token çš„ "raw" é¢„æµ‹å¾—åˆ†
        logits = outputs['scores'][0][0]    #The first token
        # print(logits)
        # print(logits[tokenizer("A").input_ids[0]])
        # probs æ˜¯ä¸€ä¸ªåŒ…å«å››ä¸ªé€‰é¡¹ Aã€Bã€Cã€D å¯¹åº”çš„æ¦‚ç‡åˆ†å¸ƒã€‚åŒ…å«å››ä¸ªæµ®ç‚¹æ•°çš„æ•°ç»„
        probs = (
            torch.nn.functional.softmax(
                # å¯¹ logits è¿›è¡Œ softmax è½¬æ¢ï¼Œå°† logits è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œ
                # dim=0 è¡¨ç¤ºåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šåº”ç”¨ softmaxï¼Œå³å¯¹æ‰€æœ‰é€‰é¡¹çš„ logits è¿›è¡Œå½’ä¸€åŒ–
                torch.tensor(
                    [
                        # tokenizer å°†æ¯ä¸ªé€‰é¡¹ è½¬åŒ–ä¸ºå¯¹åº”çš„ token ID
                        # é€šè¿‡ç´¢å¼•è·å–ç»™å®šé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰å¯¹åº”çš„ logits åˆ†æ•°ã€‚
                        logits[tokenizer("A").input_ids[0]],
                        logits[tokenizer("B").input_ids[0]],
                        logits[tokenizer("C").input_ids[0]],
                        logits[tokenizer("D").input_ids[0]],
                    ]
                ),
                dim=0,
            )
            # å°† PyTorch çš„ tensor è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå¹¶ç¡®ä¿ä¸ä¼šæœ‰æ¢¯åº¦è®¡ç®—ï¼ˆdetachï¼‰ï¼Œå¹¶å°†å…¶ä» GPU ç§»åŠ¨åˆ° CPU ä¸Š
            .detach().cpu().numpy()
        )
        # print(probs)
        # å–æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºç´¢å¼•ï¼Œæ˜ å°„åˆ°å­—æ¯ä½œä¸º æ¨ç†è¾“å‡º
        output_text = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]

    elif args.model == "THUDM/chatglm2-6b":
        response, history = model.chat(tokenizer, full_input, history=[])
        # chatglm2-6b çš„å›å¤ä¸€èˆ¬æ˜¯ [é€‰é¡¹]'.' [é€‰é¡¹å†…å®¹]. [ç†ç”±]
        output_text = response.split('.')[0]

    elif args.model == "Qwen/Qwen-7B":
        inputs = tokenizer(full_input,return_tensors="pt").to(0)
        # print(inputs.keys())
        outputs = model.generate(
                # è¿™é‡Œå±•å¼€äº† 'input_ids' 'attention_mask' ç­‰ä¿¡æ¯
                **inputs,
                max_new_tokens = 1,
                output_scores = True,   # è¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„æ¯ä¸ª token çš„åˆ†æ•°ï¼ˆlogitsï¼‰
                return_dict_in_generate=True    # ä»¥å­—å…¸å½¢å¼è¿”å›ç”Ÿæˆçš„è¾“å‡ºï¼Œå…¶ä¸­åŒ…æ‹¬ç”Ÿæˆçš„ tokens å’Œ logits
            )
        # print(outputs.keys())
        # print(outputs['sequences'])
        # print(tokenizer.decode(outputs['sequences'][0][length:]))
        logits = outputs['scores'][0][0]
        # print(logits[tokenizer("A").input_ids[0]])
        probs = (
            torch.nn.functional.softmax(
                # å¯¹ logits è¿›è¡Œ softmax è½¬æ¢ï¼Œå°† logits è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œ
                # dim=0 è¡¨ç¤ºåœ¨ç¬¬ä¸€ä¸ªç»´åº¦ä¸Šåº”ç”¨ softmaxï¼Œå³å¯¹æ‰€æœ‰é€‰é¡¹çš„ logits è¿›è¡Œå½’ä¸€åŒ–
                torch.tensor(
                    [
                        # tokenizer å°†æ¯ä¸ªé€‰é¡¹ è½¬åŒ–ä¸ºå¯¹åº”çš„ token ID
                        # é€šè¿‡ç´¢å¼•è·å–ç»™å®šé€‰é¡¹ï¼ˆAã€Bã€Cã€Dï¼‰å¯¹åº”çš„ logits åˆ†æ•°ã€‚
                        logits[tokenizer("A").input_ids[0]],
                        logits[tokenizer("B").input_ids[0]],
                        logits[tokenizer("C").input_ids[0]],
                        logits[tokenizer("D").input_ids[0]],
                    ],
                    # NOTE Qwen çš„å‘é‡æ ¼å¼æ˜¯ bf16ï¼Œéœ€è¦è½¬æ¢
                    dtype=torch.float32
                ),
                dim=0,
            )
            # å°† PyTorch çš„ tensor è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œå¹¶ç¡®ä¿ä¸ä¼šæœ‰æ¢¯åº¦è®¡ç®—ï¼ˆdetachï¼‰ï¼Œå¹¶å°†å…¶ä» GPU ç§»åŠ¨åˆ° CPU ä¸Š
            .detach().cpu().numpy()
        )
        # print(probs)
        # å–æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºç´¢å¼•ï¼Œæ˜ å°„åˆ°å­—æ¯ä½œä¸º æ¨ç†è¾“å‡º
        output_text = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
    
    elif args.model == "Qwen/Qwen2.5-3B":
        # FIXME æ³¨æ„è¿™é‡Œ Qwen2.5-3B ç”¨çš„ prompt å’Œå…¶ä»–æ¨¡å‹ä¸ä¸€è‡´
        messages = [
            {"role": "system", "content": f"You are an expert on{s}. You must answer me first and then give me your reasons."},
            {"role": "user", "content": full_input},
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=512,
            # FIXME æ˜¾å¼æŒ‡å®š pad_token é¿å…æ§åˆ¶å°æ˜¾ç¤º Setting pad_token_id to eos_token_id:151643 for open-end generation
            pad_token_id=0
        )
        generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # print(response)
        tmpList = response.split('.')
        output_text = f"{tmpList[0]}. {tmpList[1]}"
        # print(output_text)
    
    elif args.model == "Qwen/Qwen2-1.5B-Instruct" or args.model == "Qwen/Qwen2.5-3B-Instruct":
        messages = [
            {"role": "system", "content": f"You are an expert on {s}. Just give your answer between A, B, C, D, don't say anything else."},
            {"role": "user", "content": full_input}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

        outputs = model.generate(
            **model_inputs,
            max_new_tokens=10,
            temperature=0.1,
            # FIXME æ˜¾å¼æŒ‡å®š pad_token é¿å…æ§åˆ¶å°æ˜¾ç¤º Setting pad_token_id to eos_token_id:151643 for open-end generation
            # pad_token_id=0,
            output_scores= True,
            return_dict_in_generate=True
        )

        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs['sequences'])
        ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # print(response)

        # logits ä¸ºæ¨¡å‹è¾“å‡ºç¬¬ 1 ä¸ª token çš„å„ç§å¯èƒ½çš„ raw é¢„æµ‹åˆ†æ•°
        logits = outputs['scores'][0][0]
        probs = (
            torch.nn.functional.softmax(
                torch.tensor(
                    [
                        logits[tokenizer("A").input_ids[0]],
                        logits[tokenizer("B").input_ids[0]],
                        logits[tokenizer("C").input_ids[0]],
                        logits[tokenizer("D").input_ids[0]],
                    ]
                ),
                dim=0,
            )
            .detach()
            .cpu()
            .numpy()
        )
        output_text = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
        return output_text, probs, response


    return output_text


if __name__ == "__main__":
    print(os.getcwd())
    
    parser = ArgumentParser()
    # NOTE è®­ç»ƒé›†é»˜è®¤ä½¿ç”¨ MMLU_ID_train.json
    parser.add_argument('--dataset', type=str, default="MMLU_ID_train")
    parser.add_argument('--prompt', type=str, default="ID",choices=["ID","OOD"])
    parser.add_argument('--model', type=str, required=True)
    
    args = parser.parse_args()
    
    tokenizer, model = get_TOKENIZER_and_MODEL(args.model)

    # print(f"================æ¨¡å‹è®¾å¤‡æ£€æŸ¥: {model.device}================")
    # exit(0)

    LMFlow_data = {"type":"text_only","instances":[]}

    # ç”¨äº LMFlow çš„å¾®è°ƒæ•°æ®
    training_data = []
    LMFlow_data = {"type":"text_only","instances":[]}
    # ç”¨äº Llama Factory çš„å¾®è°ƒæ•°æ®
    LlamaFactory_data = []
    # ç”¨äºæµ‹è¯•æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„æ•°æ®
    data = []
    # ç”¨äºæµ‹è¯•æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„ fewshot æ•°æ®
    prompt = []

    # è¯»å–æ•°æ®KnowledgeBoundary/data/C-Eval/5-shots.json
    with open(f"./data/MMLU/{args.dataset}.json",'r') as f:
        data = json.load(f)
    # NOTE åˆå§‹æç¤ºç¬¦é»˜è®¤ä½¿ç”¨ MMLU_ID_prompt.json
    with open(f"./data/MMLU/MMLU_{args.prompt}_prompt.json",'r') as f:
        prompt = json.load(f)
    
    print(f"ğŸŸMMLU datasets num of domains: {len(data)}")

    # ç»Ÿè®¡é€šè¿‡ç‡
    Calcu_PASS = {}
    TOTAL, PASS = 0, 0
    # ç»Ÿè®¡æ¯ä¸€ä¸ªé—®é¢˜çš„ â€œæ­£ç¡®æ€§â€Cor å’Œ â€œç¡®å®šæ€§â€Cer
    CORCER={}
    texttmp = ""
    anstmp = ""

    
    # NOTE éå† MMLU çš„å„ä¸ªé¢†åŸŸï¼ˆMMLU æ˜¯ä¸€ä¸ªå­—å…¸ï¼‰
    for domain in tqdm(data.keys()):
        # if domain != "abstract_algebra":
        #     continue
        # åˆ†é¢†åŸŸç»Ÿè®¡
        Calcu_PASS[domain] = {
            "PASS": 0,
            "TOTAL": 0,
            "ACC": 0.0000
        }
        CORCER[domain] = {}

        # NOTE å„ä¸ªå­é¢†åŸŸçš„ value æ˜¯ä¸€ä¸ª list[list[str]]
        # sample æ˜¯ list[str] çš„å¤šé€‰é¢˜æ•°æ®ï¼šQuestion, Option1, Option2, Option3, Option4, Answer
        for sample in tqdm(data[domain]):
            # åˆå§‹åŒ–é—®é¢˜çš„â€œæ­£ç¡®æ€§â€å’Œâ€œç¡®å®šæ€§â€
            CORCER[domain][sample[0]] = {
                "COR": 0.0000,
                "CER": 0.0000
            }

            full_input = gen_prompt(sample, domain, prompt[domain])
            output, probs, _ = inference(tokenizer, model, full_input, domain)
            
            text = full_input
            texttmp = format_example(sample)
            # å¦‚æœæ¨¡å‹è¾“å‡ºçš„ç­”æ¡ˆåœ¨æ ‡å‡†ç­”æ¡ˆä¸­ï¼Œåˆ™è®¤ä¸ºå›ç­”æ­£ç¡®
            if sample[5] in output:
                anstmp = sample[5]
                text += f"{sample[5]}."
                Calcu_PASS[domain]["PASS"] += 1
                PASS += 1
                # ç»Ÿè®¡é—®é¢˜çš„â€œæ­£ç¡®æ€§â€ï¼šæ¨¡å‹ç»™å‡º æ­£ç¡®å›ç­”çš„æ¦‚ç‡
                # NOTE è¿™é‡Œå…¶å®å°±è¿›è¡Œäº†ä¸€éƒ¨åˆ†CorCer-RAIT Figure 4(c) å¯¹äºå·¦ä¸Šè§’ [D1_drop] çš„åˆ é™¤ï¼Œå¦‚æœæ¨¡å‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ä¸æ˜¯æœ€é«˜ï¼Œæˆ‘ä»¬è®°ä¸º 0ï¼Œé»˜è®¤å®ƒä½äºé˜ˆå€¼ Ï„
                # è¿™æ ·è¯•å›¾é¿å… é”™è¯¯é›† ç»è¿‡å¾®è°ƒ è¿›å…¥æ­£ç¡®é›†ï¼Œäº§ç”ŸåŠ¨æ€å†²çª
                CORCER[domain][sample[0]]["COR"] = probs[np.argmax(probs)].astype(float)
            # å¦åˆ™è®¤ä¸ºå›ç­”é”™è¯¯ã€‚
            # å›ç­”é”™è¯¯ï¼Œå³ä¸ºä¸ç¡®å®šunsureï¼Œæˆ‘ä»¬å¸Œæœ›è®­ç»ƒæ¨¡å‹æ‹’ç»å›ç­”ï¼Œç”¨ N è¡¨ç¤º
            else:
                text += "N." 
                anstmp = "N"

            training_data.append({"text": text})
                
            LlamaFactory_data.append({
                "instruction": "Output as N means the knowledge you are not sure about,and output as one of A, B, C, D means the knowledge you are certain about.",
                "input": texttmp,
                "output": anstmp
            })

            # ç»Ÿè®¡é—®é¢˜çš„â€œç¡®å®šæ€§â€ï¼Œæ³¨æ„å°† float32 è½¬åŒ–ä¸º floatï¼Œä¸ç„¶ JSON ä¸æ”¯æŒ
            # å½“å­˜åœ¨ 0 æ—¶ è½¬æ¢ä¸ºéå¸¸å°çš„æ•°å­—ï¼Œé¿å… log(0) æ— ç©·å¤§
            np_probs = np.array(probs)
            np_probs = np.where(np_probs == 0, 1e-9, np_probs)
            log_probs = np.log(np_probs)
            # è®¡ç®—äº¤å‰ç†µ
            CORCER[domain][sample[0]]["CER"] = -np.sum(np_probs * log_probs).astype(float)

            if np.isnan(CORCER[domain][sample[0]]["CER"]) or np.isnan(CORCER[domain][sample[0]]["COR"]):
                print(f"âš  Error during inference: {CORCER[domain][sample[0]]}\nå‘ç”Ÿé”™è¯¯çš„é—®é¢˜æ˜¯{sample[0]}\n")
                print(_)
                print(np_probs)
                print(log_probs)
                exit(0)
            
            # ç»Ÿè®¡é¢†åŸŸé—®é¢˜æ•° å’Œ æ€»é—®é¢˜æ•°
            Calcu_PASS[domain]["TOTAL"] += 1
            TOTAL += 1

        # è®¡ç®—é¢†åŸŸé€šè¿‡ç‡ 
        Calcu_PASS[domain]["ACC"] = round(Calcu_PASS[domain]["PASS"] / Calcu_PASS[domain]["TOTAL"], 4)

    # exit(0)

    model_name = f"{args.model}".split('/')[-1]

    # å¯¼å‡º LMFlow çš„å¾®è°ƒæ•°æ®
    random.shuffle(training_data)
    LMFlow_data['instances'] = training_data

    os.makedirs("./training_data", exist_ok=True)
    os.makedirs(f"./training_data/{model_name}", exist_ok=True)
    with open(f"./training_data/{model_name}/MMLU_LMFlow.json",'w') as f:
        json.dump(LMFlow_data, f)
    # å¯¼å‡º Llama Factory çš„å¾®è°ƒæ•°æ®
    with open(f"./training_data/{model_name}/MMLU_LF.json",'w') as f:
        json.dump(LlamaFactory_data, f)

    # å¯¼å‡ºæ¨¡å‹é€šè¿‡ç‡ç»Ÿè®¡ç»“æœã€çŸ¥è¯†è¾¹ç•Œã€‘
    Calcu_PASS["Final_Evaluation"] = {
        "Pass": PASS,
        "Total": TOTAL,
        "Accuarcy": round(PASS/TOTAL, 4)
    }
    os.makedirs("./2.1_evalution_res", exist_ok=True)
    os.makedirs(f"./2.1_evalution_res/{model_name}", exist_ok=True)
    with open(f"./2.1_evalution_res/{model_name}/MMLU_Pass.json", "w") as f:
        json.dump(Calcu_PASS, f)

    # å¯¼å‡ºé—®é¢˜çš„â€œæ­£ç¡®æ€§â€å’Œâ€œç¡®å®šæ€§â€ç»Ÿè®¡ç»“æœ
    with open(f"./2.1_evalution_res/{model_name}/MMLU_CORCER.json", "w") as f:
        json.dump(CORCER, f)

