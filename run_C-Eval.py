import json
import os
import random
from argparse import ArgumentParser
from sys import exit

import numpy as np
import torch
from tqdm.auto import tqdm
# å¼•å…¥è‡ªå®šä¹‰å‡½æ•°
from constants import get_TOKENIZER_and_MODEL

choices = ["A", "B", "C", "D"]


def format_example(input_list):
    prompt = input_list[0]
    k = len(input_list) - 2
    for j in range(k):
        prompt += f"\n{choices[j]}. {input_list[j+1]}"
    prompt += "\nAnswer:"
    return prompt


def format_shots(prompt_data):
    prompt = ""
    for data in prompt_data:
        prompt += data[0]       # fewshot çš„ Question
        k = len(data) - 3       # fewshot ä¸­ä½œä¸ºé€‰é¡¹çš„ Answer çš„ä¸ªæ•° NOTE C-Eval æœ‰ explanation æ‰€ä»¥-3
        for j in range(k):      # æ ¼å¼åŒ–åŠ å…¥ A B C D å››ä¸ªé€‰é¡¹
            prompt += f"\n{choices[j]}. {data[j+1]}"
        prompt += "\nAnswer:"
        prompt += data[k+1] + "\n\n"

    return prompt


def gen_prompt(input_list: list[str], subject:str, prompt_data: list[list[str]]):
    """ åŸºäº C-Eval æ•°æ®ç”Ÿæˆ prompt
    è¿™ä¸ª prompt æ˜¯é’ˆå¯¹ â€œå¦‚æœæ¨¡å‹è¿˜æ²¡æœ‰è¢«è°ƒæˆä¸€ä¸ª chatbotâ€ çš„æƒ…å†µã€‚æ‰€ä»¥ä¸åº”è¯¥åŠ å…¥ æŒ‡ä»¤æ€§è¯­å¥
    è€Œæ˜¯ä»¥ fewshots çš„å½¢å¼è®©æ¨¡å‹è¿›è¡Œ text completion/genertion
    Params:
        input_text: C-Eval çš„å¤šé€‰é¢˜æ•°æ® Question, Option1, Option2, Option3, Option4, Answer
        subject: C-Eval çš„ keyâ€”â€”é¢†åŸŸåç§°
        prompt_data: 5-shots.json åœ¨ subject é¢†åŸŸä¸‹çš„å†…å®¹
    """
    # NOTE é¢†åŸŸä»‹ç»
    # prompt = f"The following are multiple choice questions (with answers) about{subject}.\n\n"
    prompt = ""
    # NOTE fewshot æ„å»º
    prompt += format_shots(prompt_data)
    # NOTE é—®é¢˜åŠ å…¥
    prompt += format_example(input_list)
    return prompt


def inference(tokenizer, model, full_input, subject):
    
    if args.model == "Qwen/Qwen2-1.5B-Instruct" or args.model == "Qwen/Qwen2.5-3B-Instruct":
        messages = [
            {"role": "system", "content": f"You are an expert on {subject}. Just give your answer between A, B, C, D, don't say anything else."},
            # {"role": "system", "content": f"ä½ æ˜¯ {subject} é¢†åŸŸçš„ä¸“å®¶. ä½ éœ€è¦å¯¹ç”¨æˆ·ç»™å‡ºçš„æœ€åä¸€ä¸ªé—®é¢˜é€‰æ‹©ä¸€ä¸ªç­”æ¡ˆã€‚è¯·ç¡®ä¿ä½ ä¸€å®šå…ˆå›ç­”é€‰é¡¹ã€‚"},
            {"role": "user", "content": full_input}
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        
        outputs = model.generate(
            **model_inputs,
            max_new_tokens=10,
            temperature=0.1,
            # FIXME æ˜¾å¼æŒ‡å®š pad_token é¿å…æ§åˆ¶å°æ˜¾ç¤º Setting pad_token_id to eos_token_id:151643 for open-end generation
            # pad_token_id=0,
            output_scores= True,
            return_dict_in_generate=True
        )
        # print(outputs.keys()) # dict_keys(['sequences', 'scores', 'past_key_values'])
        generated_ids = [
            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, outputs['sequences'])
        ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # print(response)

        # logits ä¸ºæ¨¡å‹è¾“å‡ºç¬¬ 1 ä¸ª token çš„å„ç§å¯èƒ½çš„ raw é¢„æµ‹åˆ†æ•°
        logits = outputs['scores'][0][0]
        probs = (
            torch.nn.functional.softmax(
                torch.tensor(
                    [
                        logits[tokenizer("A").input_ids[0]],
                        logits[tokenizer("B").input_ids[0]],
                        logits[tokenizer("C").input_ids[0]],
                        logits[tokenizer("D").input_ids[0]],
                    ]
                ),
                dim=0,
            )
            .detach()
            .cpu()
            .numpy()
        )
        output_text = {0: "A", 1: "B", 2: "C", 3: "D"}[np.argmax(probs)]
        return output_text, probs, response

    raise NotImplementedError(f"Model {args.model} not supported for inference.")


if __name__ == "__main__":
    parser = ArgumentParser()
    # æ•°æ®é›†ï¼Œè¿™é‡Œé»˜è®¤æ˜¯ è®­ç»ƒé›†
    parser.add_argument('--dataset', type=str, default="train")
    parser.add_argument('--prompt', type=str, default="5-shots")
    parser.add_argument('--model', type=str, required=True)

    args = parser.parse_args()
    
    tokenizer, model = get_TOKENIZER_and_MODEL(args.model)

    # ç”¨äº LMFlow çš„å¾®è°ƒæ•°æ®
    training_data = []
    LMFlow_data = {"type":"text_only","instances":[]}
    # ç”¨äº Llama Factory çš„å¾®è°ƒæ•°æ®
    LlamaFactory_data = []
    # ç”¨äºæµ‹è¯•æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„æ•°æ®
    data = []
    # ç”¨äºæµ‹è¯•æ¨¡å‹çŸ¥è¯†è¾¹ç•Œçš„ fewshot æ•°æ®
    prompt = []

    # è¯»å–æ•°æ®KnowledgeBoundary/data/C-Eval/5-shots.json
    with open(f"./data/C-Eval/{args.dataset}.json",'r') as f:
        data = json.load(f)
    
    with open(f"./data/C-Eval/{args.prompt}.json",'r') as f:
        prompt = json.load(f)

    print(f"ğŸŸC-Eval datasets num of domains: {len(data)}")

    # ç»Ÿè®¡é€šè¿‡ç‡
    Calcu_PASS = {}
    TOTAL, PASS = 0, 0
    # ç»Ÿè®¡æ¯ä¸€ä¸ªé—®é¢˜çš„ â€œæ­£ç¡®æ€§â€Cor å’Œ â€œç¡®å®šæ€§â€Cer
    CORCER={}
    texttmp = ""
    anstmp = ""

    for domain in tqdm(data.keys()):
        # if domain != "operating_system":
        #     continue
        # åˆ†é¢†åŸŸç»Ÿè®¡
        Calcu_PASS[domain] = {
            "PASS": 0,
            "TOTAL": 0,
            "ACC": 0.0000
        }
        CORCER[domain] = {}

        for sample in tqdm(data[domain]):
            # åˆå§‹åŒ–é—®é¢˜çš„â€œæ­£ç¡®æ€§â€å’Œâ€œç¡®å®šæ€§â€
            CORCER[domain][sample[0]] = {
                "COR": 0.0000,
                "CER": 0.0000
            }

            full_input = gen_prompt(sample, domain, prompt[domain])
            output, probs, _ = inference(tokenizer, model, full_input, domain)
            
            text = full_input
            texttmp = format_example(sample)
            # å¦‚æœæ¨¡å‹è¾“å‡ºçš„ç­”æ¡ˆåœ¨æ ‡å‡†ç­”æ¡ˆä¸­ï¼Œåˆ™è®¤ä¸ºå›ç­”æ­£ç¡®
            if sample[5] in output:
                anstmp = sample[5]
                text += f"{sample[5]}."
                Calcu_PASS[domain]["PASS"] += 1
                PASS += 1
                # ç»Ÿè®¡é—®é¢˜çš„â€œæ­£ç¡®æ€§â€ï¼šæ¨¡å‹ç»™å‡º æ­£ç¡®å›ç­”çš„æ¦‚ç‡
                # NOTE è¿™é‡Œå…¶å®å°±è¿›è¡Œäº†ä¸€éƒ¨åˆ†CorCer-RAIT Figure 4(c) å¯¹äºå·¦ä¸Šè§’ [D1_drop] çš„åˆ é™¤ï¼Œå¦‚æœæ¨¡å‹ç»™å‡ºæ­£ç¡®ç­”æ¡ˆçš„æ¦‚ç‡ä¸æ˜¯æœ€é«˜ï¼Œæˆ‘ä»¬è®°ä¸º 0ï¼Œé»˜è®¤å®ƒä½äºé˜ˆå€¼ Ï„
                # è¿™æ ·è¯•å›¾é¿å… é”™è¯¯é›† ç»è¿‡å¾®è°ƒ è¿›å…¥æ­£ç¡®é›†ï¼Œäº§ç”ŸåŠ¨æ€å†²çª
                CORCER[domain][sample[0]]["COR"] = probs[np.argmax(probs)].astype(float)
            # å¦åˆ™è®¤ä¸ºå›ç­”é”™è¯¯ã€‚
            # å›ç­”é”™è¯¯ï¼Œå³ä¸ºä¸ç¡®å®šunsureï¼Œæˆ‘ä»¬å¸Œæœ›è®­ç»ƒæ¨¡å‹æ‹’ç»å›ç­”ï¼Œç”¨ N è¡¨ç¤º
            else:
                text += "N." 
                anstmp = "N"

            training_data.append({"text": text})
                
            LlamaFactory_data.append({
                "instruction": "Output as N means the knowledge you are not sure about,and output as one of A, B, C, D means the knowledge you are certain about.",
                "input": texttmp,
                "output": anstmp
            })

            # ç»Ÿè®¡é—®é¢˜çš„â€œç¡®å®šæ€§â€ï¼Œæ³¨æ„å°† float32 è½¬åŒ–ä¸º floatï¼Œä¸ç„¶ JSON ä¸æ”¯æŒ
            # å½“å­˜åœ¨ 0 æ—¶ è½¬æ¢ä¸ºéå¸¸å°çš„æ•°å­—ï¼Œé¿å… log(0) æ— ç©·å¤§
            np_probs = np.array(probs)
            np_probs = np.where(np_probs == 0, 1e-9, np_probs)
            log_probs = np.log(np_probs)
            # è®¡ç®—äº¤å‰ç†µ
            CORCER[domain][sample[0]]["CER"] = -np.sum(np_probs * log_probs).astype(float)

            if np.isnan(CORCER[domain][sample[0]]["CER"]) or np.isnan(CORCER[domain][sample[0]]["COR"]):
                print(f"âš  Error during inference: {CORCER[domain][sample[0]]}\nå‘ç”Ÿé”™è¯¯çš„é—®é¢˜æ˜¯{sample[0]}\n")
                print(_)
                print(np_probs)
                print(log_probs)
                exit(0)
            
            # ç»Ÿè®¡é¢†åŸŸé—®é¢˜æ•° å’Œ æ€»é—®é¢˜æ•°
            Calcu_PASS[domain]["TOTAL"] += 1
            TOTAL += 1

        # è®¡ç®—é¢†åŸŸé€šè¿‡ç‡ 
        Calcu_PASS[domain]["ACC"] = round(Calcu_PASS[domain]["PASS"] / Calcu_PASS[domain]["TOTAL"], 4)

    # exit(0)

    model_name = f"{args.model}".split('/')[-1]

    # å¯¼å‡º LMFlow çš„å¾®è°ƒæ•°æ®
    random.shuffle(training_data)
    LMFlow_data['instances'] = training_data

    os.makedirs("./training_data", exist_ok=True)
    os.makedirs(f"./training_data/{model_name}", exist_ok=True)
    with open(f"./training_data/{model_name}/C-Eval_LMFlow.json",'w') as f:
        json.dump(LMFlow_data, f)
    # å¯¼å‡º Llama Factory çš„å¾®è°ƒæ•°æ®
    with open(f"./training_data/{model_name}/C-Eval_LF.json",'w') as f:
        json.dump(LlamaFactory_data, f)

    # å¯¼å‡ºæ¨¡å‹é€šè¿‡ç‡ç»Ÿè®¡ç»“æœã€çŸ¥è¯†è¾¹ç•Œã€‘
    Calcu_PASS["Final_Evaluation"] = {
        "Pass": PASS,
        "Total": TOTAL,
        "Accuarcy": round(PASS/TOTAL, 4)
    }
    os.makedirs("./2.1_evalution_res", exist_ok=True)
    os.makedirs(f"./2.1_evalution_res/{model_name}", exist_ok=True)
    with open(f"./2.1_evalution_res/{model_name}/C-Eval_Pass.json", "w") as f:
        json.dump(Calcu_PASS, f)

    # å¯¼å‡ºé—®é¢˜çš„â€œæ­£ç¡®æ€§â€å’Œâ€œç¡®å®šæ€§â€ç»Ÿè®¡ç»“æœ
    with open(f"./2.1_evalution_res/{model_name}/C-Eval_CORCER.json", "w") as f:
        json.dump(CORCER, f)