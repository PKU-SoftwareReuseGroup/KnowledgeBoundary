import torch
import json
from tqdm.auto import tqdm
import random
from argparse import ArgumentParser
from scipy.stats import entropy
import math
import os

# å¼•å…¥è‡ªå®šä¹‰å˜é‡
from constants import client, get_TOKENIZER_and_MODEL
# å¼•å…¥ exit ç”¨äºè°ƒè¯•
from sys import exit


def gen_prompt(input_text: str):
    if args.model == "openlm-research/open_llama_3b":
        return "Question:" + input_text + " Answer:"
    # å…¶ä»–æ¨¡å‹ç”¨æ¯”è¾ƒå¼ºçš„æç¤º
    return f"Now I have a Question: {input_text}\nPlease answer the question based on your internal knowledge: "


def inference(full_input: str):
    """
    Params:
        tokenizer: åˆ†è¯å™¨
        model: ç”¨äºæ¨ç†çš„æ¨¡å‹
        full_input: 0-shot prompt å¼€æ”¾åŸŸçŸ¥è¯†é—®ç­”æ²¡æœ‰ä¾‹å­
    """
    output_text = ""
    if args.model == "openlm-research/open_llama_3b":
        inputs = tokenizer(full_input,return_tensors="pt").to(0)
        ids = inputs['input_ids']
        length = len(ids[0])
        
        outputs = model.generate(
                ids,
                #temperature=0.7,
                #do_sample = True,
                max_new_tokens = 15,
            )
        # FIXME å› ä¸ºè¿™ä¸ªæ¨¡å‹æ˜¯ä¸€ä¸ªè¡¥å…¨æ¨¡å‹ï¼Œç”Ÿæˆçš„å†…å®¹æ˜¯æ¥ç»­åœ¨è¾“å…¥ä¹‹åï¼Œæ‰€ä»¥è¦ä»è¿™ä»¥å æˆªå–è¾“å‡º
        output_text = tokenizer.decode(outputs[0][length:])
        # print(f"æ•°æ®åº“é—®é¢˜\n{full_input}")
        # print(f"æ¨¡å‹å¯¹æ•°æ®åº“é—®é¢˜å®Œæ•´çš„å›ç­”\n{output_text}")
        idx = output_text.find('.')
        output_text = output_text[:idx]

    elif args.model == "THUDM/chatglm2-6b":
        response, history = model.chat(tokenizer, full_input, history=[])
        # print(f"response: {response}")
        output_text = response.split('.')[0] + "."

    elif args.model == "Qwen/Qwen2.5-3B" or args.model == "Qwen/Qwen2-1.5B-Instruct":
        messages = [
            {"role": "system", "content": f"You are a Knowledge Q&A expert."},
            {"role": "user", "content": full_input},
        ]
        text = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        model_inputs = tokenizer([text], return_tensors="pt").to(model.device)
        generated_ids = model.generate(
            **model_inputs,
            max_new_tokens=32,
            # FIXME æ˜¾å¼æŒ‡å®š pad_token é¿å…æ§åˆ¶å°æ˜¾ç¤º Setting pad_token_id to eos_token_id:151643 for open-end generation
            pad_token_id=0
        )
        generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ]
        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        # print(response)
        output_text = response.split('.')[0] + "."

    return output_text


def judge_answer_similarity(q: str, a1: str, a2: str):
    full_input=f"""é—®é¢˜: {q}
å›ç­”1: {a1}
å›ç­”2: {a2}
è¯´æ˜: å›ç­”1 æ˜¯é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆã€‚å›ç­”2 å¯èƒ½ä¼šç»™å‡ºä¸€äº›å¤šä½™çš„ä¿¡æ¯ã€‚
å¦‚æœ å›ç­”2 ä¸­çš„ä¿¡æ¯å’Œ å›ç­”1 å«ä¹‰ç›¸ä¼¼ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰ï¼Œæˆ–è€…ä»å¦ä¸€ä¸ªæ–¹é¢å›ç­”äº†å’Œ å›ç­”1 çš„ç›¸ä¼¼çš„å«ä¹‰ï¼Œé‚£ä¹ˆæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯åˆæ ¼çš„ã€‚
å¦‚æœ å›ç­”2 å¯¹äº é—®é¢˜ çš„å›ç­” å’Œ å›ç­”1 æœ‰è¾ƒå¤§çš„å·®å¼‚ï¼Œæˆ–è€… å›ç­”2 ç»™å‡ºçš„ç­”æ¡ˆè¿‡åº¦å®½æ³›ï¼Œé‚£ä¹ˆæˆ‘ä»¬è®¤ä¸ºå®ƒæ˜¯ä¸åˆæ ¼çš„ã€‚
æ³¨æ„: å›ç­”2 æ˜¯å¦æ˜¯ä¸€ä¸ªå®Œæ•´çš„å¥å­ï¼Œæ˜¯å¦åŒ…å«äº†å…¶ä»–ä¿¡æ¯ï¼Œ**ä¸ä½œä¸ºå…¶æ˜¯å¦åˆæ ¼çš„åˆ¤æ–­ä¾æ®**
ä»»åŠ¡: åˆ¤æ–­å›ç­”2çš„æ­£ç¡®æ€§ï¼Œå›ç­” YES æˆ–è€… NOï¼Œå¹¶å¦èµ·ä¸€è¡Œ **ç®€è¦** ç»™å‡ºåˆ¤æ–­ç†ç”±ã€‚

ä¸‹é¢æ˜¯ä¸€äº›ä¾‹å­ï¼Œè¯·å‚è€ƒåå®Œæˆä»»åŠ¡ã€‚

### ä¾‹å­1
é—®é¢˜: In what field does Anaxagoras work?
å›ç­”1: philosophy
å›ç­”2: Anaxagoras was a Greek philosopher who lived in the
å‚è€ƒåˆ¤æ–­: 
YES
å›ç­”2 æä¾›äº† Anaxagoras æ˜¯å¸Œè…Šå“²å­¦å®¶çš„äº‹å®ï¼Œä»å¦ä¸€ä¸ªè§’åº¦å›ç­”äº† Anaxagoras åœ¨å“ªä¸€ä¸ªé¢†åŸŸå·¥ä½œã€‚å› æ­¤ï¼Œå›ç­”2 ä¸­çš„ä¿¡æ¯å’Œ å›ç­”1 ç›¸ä¼¼ï¼Œæ‰€ä»¥åˆ¤æ–­åˆæ ¼ã€‚

### ä¾‹å­2
é—®é¢˜: What field does Robert Bunsen work in?
å›ç­”1: chemistry
å›ç­”2: Bunsen is a German chemist who is best known for his inventio
å‚è€ƒåˆ¤æ–­: 
YES
å›ç­”2 æä¾›äº† Bunsen æ˜¯å¾·å›½åŒ–å­¦å®¶çš„äº‹å®ï¼Œä»å¦ä¸€ä¸ªè§’åº¦å›ç­”äº† Bunsen åœ¨å“ªä¸€ä¸ªé¢†åŸŸå·¥ä½œã€‚å°½ç®¡ å›ç­”2 å¹¶ä¸å®Œæ•´ï¼Œå¹¶ä¸”å«æœ‰å…¶ä»–ä¿¡æ¯ï¼Œä½†æ˜¯å®ƒåŒ…å«äº†å’Œ å›ç­”1 ç›¸ä¼¼çš„ä¿¡æ¯ï¼Œæ‰€ä»¥åˆ¤æ–­åˆæ ¼ã€‚

### ä¾‹å­3
é—®é¢˜: What field does Alan Turing work in?
å›ç­”1: logic
å›ç­”2: Alan Turing is a computer scientist
å‚è€ƒåˆ¤æ–­: 
NO
å›ç­”2 æè¿° Alan Turing æ˜¯åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸå·¥ä½œï¼Œè€Œè®¡ç®—æœºç§‘å­¦é¢†åŸŸ å’Œ é€»è¾‘é¢†åŸŸä¸å­˜åœ¨ç›´æ¥çš„ä»å±å…³ç³»ï¼Œå› æ­¤ï¼Œå›ç­”2 å’Œ å›ç­”1 æœ‰è¾ƒå¤§çš„å·®å¼‚ï¼Œæ‰€ä»¥åˆ¤æ–­ä¸åˆæ ¼ã€‚
(å°½ç®¡ä»å®¢è§‚äº‹å®æ¥çœ‹ï¼ŒAlan Turing ç¡®å®åœ¨ä¸¤ä¸ªé¢†åŸŸéƒ½æœ‰è´¡çŒ®ï¼Œä½†æ˜¯æˆ‘ä»¬åªå‚ç…§ é—®é¢˜ å’Œ å›ç­”1)

### ä¾‹å­4
é—®é¢˜: What field does Bruce Perens work in?
å›ç­”1: programmer
å›ç­”2: Bruce Perens works in Software
å‚è€ƒåˆ¤æ–­:
NO
å›ç­”2 è™½ç„¶æŒ‡å‡ºäº† Bruce Perens åœ¨è½¯ä»¶é¢†åŸŸå·¥ä½œï¼Œä½†å¹¶æœªæ˜ç¡®æŒ‡å‡ºä»–çš„å…·ä½“èŒä¸šæ˜¯ç¨‹åºå‘˜ï¼Œè€Œæ˜¯æä¾›äº†ä¸€ä¸ªæœ‰äº›è¿‡äºå®½æ³›çš„é¢†åŸŸã€‚å› æ­¤ï¼Œå›ç­”2 å’Œ å›ç­”1 æœ‰è¾ƒå¤§çš„å·®å¼‚ï¼Œæ‰€ä»¥åˆ¤æ–­ä¸åˆæ ¼ã€‚
(å°½ç®¡åœ¨è½¯ä»¶é¢†åŸŸå·¥ä½œçš„äºº æœ‰è¾ƒå¤§çš„æ¦‚ç‡æœ‰ç¼–ç¨‹ç»éªŒï¼Œå¹¶ä¸” Bruce Perens ç¡®å®æ˜¯ä¸€ä¸ªç¨‹åºå‘˜ï¼Œä½†æ˜¯å›ç­”2 çš„å›å¤è¿‡äºå®½æ³›)
"""
    response = client.chat.completions.create(
        model="glm-4-air",
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè¯„åˆ¤è€…ï¼Œä½ åªä¼šåŸºäºç”¨æˆ·æä¾›çš„ä¿¡æ¯ï¼Œä¸åˆ©ç”¨ä½ è‡ªå·±æŒæ¡çš„çŸ¥è¯†ã€‚"},
            {"role": "user", "content": full_input}
        ],
        # temperature=0.3
    )
    response = response.choices[0].message.content
    # print("------- è¯„åˆ¤å›ç­”ä¸€è‡´æ€§çš„ prompt -------")
    # print(full_input)
    # print("------- è¯„åˆ¤å›ç­”ä¸€è‡´æ€§çš„ ç»“æœ -------")
    # print(response)
    idx = response.find('\n')
    response = response[:idx]
    return response


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument('--dataset', type=str, default="training_data")
    parser.add_argument('--model', type=str, required=True)
    # parser.add_argument('--result',type=str, default="pararel")
    parser.add_argument('--method',type=str,default="unsure",choices=["unsure","unknown","uncertain"])
    parser.add_argument("--num_try",type=int,default="5") #only required for uncertain method
    
    args = parser.parse_args()
    
    tokenizer, model = get_TOKENIZER_and_MODEL(args.model)

    LMFlow_data = {"type":"text_only","instances":[]}

    training_data = []
    data = []
    with open(f"./data/pararel/{args.dataset}.json",'r') as f:
        data = json.load(f)
    
    print(f"ğŸŸParaRel datasets length: {len(data)}")


    # è¿™é‡Œå­˜æ”¾çŸ¥è¯†è¾¹ç•Œè¯„ä¼°çš„ç»“æœ knowledge_boundary_eval
    ParaRel_pass, ParaRel_total = 0, 0
    # NOTE åªåœ¨è¯„ä¼°çŸ¥è¯†è¾¹ç•Œæ—¶ï¼Œç»Ÿè®¡ç»è¿‡ GLM-4-Air äºŒæ¬¡è¯„ä»·å æ¨¡å‹çš„å‡†ç¡®ç‡
    #  å¯¹äº 2.2.1 ä»»åŠ¡ï¼Œè¿˜æ˜¯ä»æ¨¡å‹å¿ è¯šåº¦çš„è§’åº¦å‡ºå‘ï¼Œç”¨ in çš„æ–¹å¼æ¥ä¸¥æ ¼é™åˆ¶
    Judge_pass = 0

    # NOTE sample[0] æ˜¯é—®é¢˜. sample[1] æ˜¯å›ç­”. è¿™é‡Œä¼¼ä¹æŠ›å¼ƒäº† sample[2]
    # for sample in tqdm(data[100:115]):
    for sample in tqdm(data):

        full_input = gen_prompt(sample[0])
        
        if args.method == "unsure":
            output = inference(full_input)
            
            text = f"Question: {sample[0]} Answer: {sample[1]}. Are you sure you accurately answered the question based on your internal knowledge?"
            # print(f"Ground Truth Q: {sample[0]}")
            # print(f"Ground Truth A: {sample[1]}")
            # print(f"Model Answer: {output}")
            # NOTE å¯¹æ¯” æ ‡å‡†å›ç­” A å’Œ æ¨¡å‹å›ç­” A' åˆ’åˆ† ä¸ç¡®å®šé›†D_0ã€ç¡®å®šé›†D_1
            # è¿™é‡ŒåŠ ä¸€ä¸ªä¸»è¦æ˜¯å› ä¸º åªå› ä¸ºé¦–å­—æ¯ä¸åŒå¯¼è‡´åˆ¤æ–­å¤±è¯¯çš„å¤ªå¤šäº†
            try:
                # if sample[1] in output or sample[1].capitalize() in output:
                if sample[1].lower() in output.lower():
                    text += " I am sure."
                    ParaRel_pass += 1
                    Judge_pass += 1
                else:
                    text += " I am unsure."
                    judge_res = judge_answer_similarity(sample[0], sample[1], output)
                    if "YES" in judge_res:
                        # text += " I am sure."
                        # ParaRel_pass += 1
                        Judge_pass += 1
                    # else:
                    #     text += " I am unsure."
            except Exception as e:
                print(f"âš  Error during inference: {e}\nå‘ç”Ÿé”™è¯¯çš„æ˜¯{sample[0]}\n{sample[1]}\n")
                # å¦‚æœé‡åˆ°æ•æ„Ÿå†…å®¹é”™è¯¯ï¼Œç›´æ¥è®¤ä¸º unsure
                text += " I am unsure."
                
            training_data.append({"text":text})
            ParaRel_total += 1

        else:
            raise Exception("ä¸æ”¯æŒçš„æ–¹æ³•")

    random.shuffle(training_data)
    LMFlow_data['instances'] = training_data

    model_name = args.model.split("/")[1]

    os.makedirs("./training_data",exist_ok=True)
    os.makedirs(f"./training_data/{model_name}", exist_ok=True)
    with open(f"./training_data/{model_name}/ParaRel_{args.method}.json",'w') as f:
        json.dump(LMFlow_data,f)

    
    KB_eval = {
        "Pass": ParaRel_pass,
        "JudgePass": Judge_pass,
        "Total": ParaRel_total,
        "Accuarcy": round(ParaRel_pass/ParaRel_total, 4),
        "JudgeAccuarcy": round(Judge_pass/ParaRel_total, 4)
    }
    os.makedirs("./2.1_evalution_res", exist_ok=True)
    os.makedirs(f"./2.1_evalution_res/{model_name}", exist_ok=True)
    with open(f"./2.1_evalution_res/{model_name}/KB_on_ParaRel.json", "w") as f:
        json.dump(KB_eval, f)

